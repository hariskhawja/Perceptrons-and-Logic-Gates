{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2641c8cf",
   "metadata": {},
   "source": [
    "# Perceptons and Logic Gates\n",
    "This project focuses on training single-neuron (Perceptron) neural networks to replicate truth tables of common logic gates.\n",
    "\n",
    "# Background\n",
    "\n",
    "### Main Procedure\n",
    "Given an input vector of values, `x`, and a weight vector, `w`, apply `dot_product(x, w)` and add a bias, `b`.\n",
    "\n",
    "This produces a multi-variable linear decision boundary for the neuron, with <code>y = w<sub>0</sub>x<sub>0</sub> + w<sub>1</sub>x<sub>1</sub> + ... + w<sub>n</sub>x<sub>n</sub> + b</code>.\n",
    "Each weight indicates the \"importance\" of each input variable coming into the neuron to determine how much it affects the resulting output. The equation represents a flat hyperplane in n-dimensional space, with a \"positive\" and \"negative\" region — the neuron is effectively using the equation to determine whether the input falls under one of these categories. The goal in machine learning is to adjust the weights and biases such that the decision boundary line represents the desired pattern more accurately.\n",
    "\n",
    "### Activation Function\n",
    "Simply utilising the main procedure will only allow for linear outputs, thus activation functions take the output, `y`, as input and produce a non-linear output, `Y`, as the final neuron output — `Y = Φ(y)`. Neurons thus become non-linear and more flexible to complex patterns. Activation functions can widely vary, but are generally functions that have clear graphically bounded regions; for example, <code>tanh<sup>-1</sup>(x)</code> and `H(x)`.\n",
    "\n",
    "### Loss Function\n",
    "Adjusting the weights and biases is achieved through the loss (or error) function `L(x)`. The essential idea is to determine the magnitude and direction of error from the current output to the real output. Typically, the more rigorous the loss function, the more precise the value correction.\n",
    "\n",
    "### Updating Weights and Bias\n",
    "Updating the weightings and biases is achived through the general mappings: <code>w<sub>i</sub> ← w<sub>i</sub> - η ∂L/∂w<sub>i</sub></code> and <code>b ← b - η ∂L/∂b</code>, where `η` is the desired learning rate. The lower the learning rate, the more precise and careful the training process becomes at the cost of resources and time. Albeit `L(x)` indicates the actual error, the partial derivates indicate the magnitude and direction of adjustment needed. In the very simple case, <code>L(x) = 1/2 (y<sub>expected</sub> - y<sub>current</sub>)<sup>2</sup> </code> and thus <code>∂L/∂w = (y<sub>current</sub> - y<sub>expected</sub>)x</code>.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18f78e9c",
   "metadata": {},
   "source": [
    "# Common Logic Gates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3821f22e",
   "metadata": {},
   "source": [
    "### Perceptron Breakdown\n",
    "The Perceptron will utilise a step function and the simple error function as the activation and loss functions, respectively. The Perceptron must be trained to replicate the behaviours of the 2-fielded `OR` and `AND` gates as well as their 3-fielded versions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a5b78",
   "metadata": {},
   "source": [
    "### Perceptron Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4bd2cf2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Python Dependencies\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perceptron Class Definition\n",
    "class Perceptron:\n",
    "    def __init__(self, input_size, learning_rate=0.1):\n",
    "        self.weights = np.zeros(input_size) # Weight Vector\n",
    "        self.bias = 0 # Bias\n",
    "        self.learning_rate = learning_rate # Learning Rate\n",
    "\n",
    "    # Activation Function: Heaviside/Step Function\n",
    "    def activation_function(self, prediction):\n",
    "        return 1 if prediction >= 0 else 0\n",
    "\n",
    "    # Loss Function: Manual Error Function\n",
    "    def loss_function(self, expected_output, prediction):\n",
    "        return expected_output - prediction\n",
    "\n",
    "    # Apply Weights and Bias on Input Vector, Then Activate\n",
    "    def predict(self, x):\n",
    "        prediction = np.dot(x, self.weights) + self.bias\n",
    "        return self.activation_function(prediction)\n",
    "\n",
    "    # Loop Through Data Epoch Times, Predict, Calculate Error, Adjust Weights, Repeat\n",
    "    def train(self, input_vectors, outputs, epochs=10):\n",
    "        for _ in range(epochs):\n",
    "            for input_vector, output in zip(input_vectors, outputs):\n",
    "                prediction = self.predict(input_vector)\n",
    "                error = self.loss_function(output, prediction)\n",
    "\n",
    "                # Weight and Bias Correction\n",
    "                self.weights += self.learning_rate * error * input_vector\n",
    "                self.bias += self.learning_rate * error\n",
    "\n",
    "# Note: notice that the loss_function uses expected_output - prediction\n",
    "# It should be the other way around according to calculus, but in the train method\n",
    "# we add instead of subtracting the adjustments to the weights, so the negatives just cancel out\n",
    "# and it works either way\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6541fa75",
   "metadata": {},
   "source": [
    "### OR Logic Gate (2-field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "52712358",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OR 0 = 0\n",
      "0 OR 1 = 1\n",
      "1 OR 0 = 1\n",
      "1 OR 1 = 1\n",
      "Weights: [0.1 0.1] Bias: -0.1\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "OR_inputs = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "OR_outputs = np.array([0, 1, 1, 1])\n",
    "\n",
    "OR_gate = Perceptron(2)\n",
    "OR_gate.train(OR_inputs, OR_outputs)\n",
    "\n",
    "# Results\n",
    "print(\"0 OR 0 =\", OR_gate.predict([0, 0]))\n",
    "print(\"0 OR 1 =\", OR_gate.predict([0, 1]))\n",
    "print(\"1 OR 0 =\", OR_gate.predict([1, 0]))\n",
    "print(\"1 OR 1 =\", OR_gate.predict([1, 1]))\n",
    "print(\"Weights:\", OR_gate.weights, \"Bias:\", OR_gate.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2e589fb",
   "metadata": {},
   "source": [
    "### AND Logic Gate (2-field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "87e81c05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0 = 0\n",
      "0 AND 1 = 0\n",
      "1 AND 0 = 0\n",
      "1 AND 1 = 1\n",
      "Weights: [0.2 0.1] Bias: -0.20000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "AND_inputs = np.array([\n",
    "    [0, 0],\n",
    "    [0, 1],\n",
    "    [1, 0],\n",
    "    [1, 1]\n",
    "])\n",
    "\n",
    "AND_outputs = np.array([0, 0, 0, 1])\n",
    "\n",
    "AND_gate = Perceptron(2)\n",
    "AND_gate.train(AND_inputs, AND_outputs)\n",
    "\n",
    "# Results\n",
    "print(\"0 AND 0 =\", AND_gate.predict([0, 0]))\n",
    "print(\"0 AND 1 =\", AND_gate.predict([0, 1]))\n",
    "print(\"1 AND 0 =\", AND_gate.predict([1, 0]))\n",
    "print(\"1 AND 1 =\", AND_gate.predict([1, 1]))\n",
    "print(\"Weights:\", AND_gate.weights, \"Bias:\", AND_gate.bias)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e427b91",
   "metadata": {},
   "source": [
    "### OR Logic Gate (3-field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "d3f50eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 OR 0 OR 0 = 0\n",
      "0 OR 0 OR 1 = 1\n",
      "0 OR 1 OR 0 = 1\n",
      "0 OR 1 OR 1 = 1\n",
      "1 OR 0 OR 0 = 1\n",
      "1 OR 0 OR 1 = 1\n",
      "1 OR 1 OR 0 = 1\n",
      "1 OR 1 OR 1 = 1\n",
      "Weights: [0.1 0.1 0.1] Bias: -0.1\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "OR_inputs_3 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1],\n",
    "])\n",
    "\n",
    "OR_outputs_3 = np.array([0, 1, 1, 1, 1, 1, 1, 1])\n",
    "\n",
    "OR_gate_3 = Perceptron(3)\n",
    "OR_gate_3.train(OR_inputs_3, OR_outputs_3)\n",
    "\n",
    "# Results\n",
    "print(\"0 OR 0 OR 0 =\", OR_gate_3.predict([0, 0, 0]))\n",
    "print(\"0 OR 0 OR 1 =\", OR_gate_3.predict([0, 0, 1]))\n",
    "print(\"0 OR 1 OR 0 =\", OR_gate_3.predict([0, 1, 0]))\n",
    "print(\"0 OR 1 OR 1 =\", OR_gate_3.predict([0, 1, 1]))\n",
    "print(\"1 OR 0 OR 0 =\", OR_gate_3.predict([1, 0, 0]))\n",
    "print(\"1 OR 0 OR 1 =\", OR_gate_3.predict([1, 0, 1]))\n",
    "print(\"1 OR 1 OR 0 =\", OR_gate_3.predict([1, 1, 0]))\n",
    "print(\"1 OR 1 OR 1 =\", OR_gate_3.predict([1, 1, 1]))\n",
    "print(\"Weights:\", OR_gate_3.weights, \"Bias:\", OR_gate_3.bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be3327e3",
   "metadata": {},
   "source": [
    "### AND Logic Gate (3-field)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b90db259",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 AND 0 AND 0 = 0\n",
      "0 AND 0 AND 1 = 0\n",
      "0 AND 1 AND 0 = 0\n",
      "0 AND 1 AND 1 = 0\n",
      "1 AND 0 AND 0 = 0\n",
      "1 AND 0 AND 1 = 0\n",
      "1 AND 1 AND 0 = 0\n",
      "1 AND 1 AND 1 = 1\n",
      "Weights: [0.1 0.1 0.1] Bias: -0.20000000000000004\n"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "AND_inputs_3 = np.array([\n",
    "    [0, 0, 0],\n",
    "    [0, 0, 1],\n",
    "    [0, 1, 0],\n",
    "    [0, 1, 1],\n",
    "    [1, 0, 0],\n",
    "    [1, 0, 1],\n",
    "    [1, 1, 0],\n",
    "    [1, 1, 1],\n",
    "])\n",
    "\n",
    "AND_outputs_3 = np.array([0, 0, 0, 0, 0, 0, 0, 1])\n",
    "\n",
    "AND_gate_3 = Perceptron(3)\n",
    "AND_gate_3.train(AND_inputs_3, AND_outputs_3)\n",
    "\n",
    "# Results\n",
    "print(\"0 AND 0 AND 0 =\", AND_gate_3.predict([0, 0, 0]))\n",
    "print(\"0 AND 0 AND 1 =\", AND_gate_3.predict([0, 0, 1]))\n",
    "print(\"0 AND 1 AND 0 =\", AND_gate_3.predict([0, 1, 0]))\n",
    "print(\"0 AND 1 AND 1 =\", AND_gate_3.predict([0, 1, 1]))\n",
    "print(\"1 AND 0 AND 0 =\", AND_gate_3.predict([1, 0, 0]))\n",
    "print(\"1 AND 0 AND 1 =\", AND_gate_3.predict([1, 0, 1]))\n",
    "print(\"1 AND 1 AND 0 =\", AND_gate_3.predict([1, 1, 0]))\n",
    "print(\"1 AND 1 AND 1 =\", AND_gate_3.predict([1, 1, 1]))\n",
    "print(\"Weights:\", AND_gate_3.weights, \"Bias:\", AND_gate_3.bias)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
